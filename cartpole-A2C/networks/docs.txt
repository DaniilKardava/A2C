activations.py - Stores classes for different activation functions. Each class has a function and derivative call. Classes with 
unique parameters do not have static methods and require initialization. 

initializers.py - Stores weight and bias initializers.

network.py - Is designed to be a general MLP. It accepts custom weight initialization, input and output dimensions, hidden layer
dimensions, custom activation functions at each layer, and optimizers for performing gradient descent. To make the algorithm specific
to the problem, users define a loss function manually. Users are responsible for expanding the objective function until left with the 
gradient of the network output. The preceding terms are passed as lambda functions which are evaluated at execution using the networks
output. 
Example: MSE Objective: (true - pred)**2. Gradient of this expression expands into 2(true-pred) * (gradient of -pred) => -2(true-pred)
must be passed by the user as a lambda function => x = lambda i: -2(true - i). The gradient of pred will be calculated in a consistent 
manner using backpropogation. 

optimizers.py - Stores optimization methods. ie: stochastic gradient descent, adam.